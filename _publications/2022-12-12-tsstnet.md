---
title: "TSSTNet: A two-stream Swin transformer network for salient object detection of no-service rail surface defects"
collection: publications
category: manuscripts
permalink: /publication/2022-12-12-tsstnet
excerpt: '**Chi Wan**<sup>†</sup>, Shuai Ma and Kechen Song<sup>*</sup>'
date: 2022-12-12
venue: 'Journal 1'
slidesurl: # 'http://academicpages.github.io/files/slides1.pdf'
paperurl: 'https://www.mdpi.com/2079-6412/12/11/1730'
citation: 'Wan C, Ma S, Song K. TSSTNet: A two-stream Swin transformer network for salient object detection of no-service rail surface defects[J]. Coatings, 2022, 12(11): 1730.'
---

The detection of no-service rail surface defects is important in the rail manufacturing process. Detection of defects can prevent significant financial losses. However, the texture and form of the defects are often very similar to the background, which makes them difficult for the human eye to distinguish. How to accurately identify rail surface defects thus poses a challenge. We introduce salient object detection through machine vision to deal with this challenge. Salient object detection locates the most “significant” areas of an image using algorithms, which constitute an integral part of machine vision inspection. However, existing saliency detection networks suffer from inaccurate positioning, poor contouring, and incomplete detection. Therefore, we propose an innovative deep learning network named Two-Stream Swin Transformer Network (TSSTNet) for salient detection of no-service rail surface defects. Specifically, we propose a two-stream encoder—one stream for feature extraction and the other for edge extraction. TSSTNet also includes a three-stream decoder, consisting of a saliency stream, edge stream, and fusion stream. For the problem of incomplete detection, we innovatively introduce the Swin Transformer to model global information. For the problem of unclear contours, we expect to deepen the understanding of the difference in depth between the foreground and background through the learning of contour maps, so the contour alignment module (CAM) is created to deal with this problem. Moreover, to make the most of multimodal information, we suggest a multi-feature fusion module (MFFM). Finally, we conducted comparative experiments with 10 state-of-the-art (SOTA) approaches on the NRSD-MN datasets, and our model performed more competitively than others on five metrics.